name: Fangdi Crawler

on:
  schedule:
    - cron: "0 1 * * *"   # 每天 UTC 01:00（北京时间 09:00）
  workflow_dispatch:       # 手动运行

jobs:
  crawl:
    runs-on: ubuntu-latest

    # ★ 使用官方 Playwright 镜像：无需每次安装浏览器
    # container:
    #   image: mcr.microsoft.com/playwright/python:v1.45.0-focal

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.11

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright asyncio
          playwright install

      - name: Run Fangdi crawler
        run: |
          python fangdi_crawler.py

      - name: Commit & Push results
        run: |
          # 1. 配置 Git 身份
          git config user.name "GitHub Action"
          git config user.email "actions@github.com"
          
          # 2. 添加生成的 CSV 文件到 Git 暂存区
          # 使用 -f 是好习惯，确保即使文件已在 .gitignore 中也能添加
          git add -f fangdi_data.csv
          
          # 3. 提交更改。如果发现没有变化 (No changes)，则继续执行，不报错
          git commit -m "Daily update $(date +'%Y-%m-%d')" || echo "No changes"
          
          # 4. 推送更改。使用 GITHUB_TOKEN 进行身份验证，无需手动设置 URL
          # GITHUB_TOKEN 默认拥有足够的权限
          git push